---
title: "Predicting Adult Income via Demographics Data"
author: "Chun-Mien (Alan) Liu, Rebecca Rosette Nanfuka, Roganci Fontelera, Yonas Gebre Marie"
jupyter: python3
format:
  html:
    toc: true
    toc-depth: 3
bibliography: references.bib
execute:
  eval: false
---


## Summary
Here, we tried to find the classification models with the highest accuracy on predicting if an individual's income is greater than 50K/yr based on census data. Our final classifier has a reasonable performance on the unseen test data. We observed the Logistic Regression test accuracy of $0.848$ and the RBF-SVM test accuracy of $0.855$.

The prediction model can possibly be further improved by pruning more features that are irrelevant to the prediction, since we are using almost all of the features for fitting the classifiers.

## Introduction
A lot of factors impact an individual's income. We see how wealth remains concentrated as the top 1% held 35% of the total wealth in 2022 (@kuhn2025income). More recently, from 2024 to 2025, the U.S. Federal Reserve shows 35% of the family income less than 50K per year (@fed2025households). It is a big topic because it involves each family's economic well-being.

This notebook explores the Adult Income dataset to understand how different demographic and employment characteristics relate to whether a person earns more or less than $50,000 per year. The analysis begins with exploring patterns in the data, identifying any issues, and preparing the dataset for modeling. After cleaning and preprocessing, we build and evaluate predictive models to see how well income levels can be predicted using the available features.

## Methods
### Data
The data set used here is from UC Irvine Machine Learning Repository, extracted by Barry Becker from the 1994 Census database (@uci_adult).

### Analysis
The algorithms below are considered for predicting whether if an individual's income was above 50K/year or not: decision tree, k-nearest neighbors (k-nn), support vector machine with RBF kernel (SVM-RBF), logistic regression, gaussian naive bayes.
All variables were used to fit the model, with the exception of:
- `fnlwgt`: useless, since each row has a unique value
- `education-num`: due to redundancy with `education`
- `race`: for ethical reasons.

Data was split with 80% into the test set, and 20% into the train set to increase training time due to the vast amount of data. Imputation, one-hot encoding, and ordinal encoding are all done accordingly with details explained below. For model selection, we first conduct 5-fold cross-validation on all the models with default hyperparameters. Then, we select the top two models (Logistic Regression and SVM-RBF) to conduct a random search of hyperparameters with 3-fold cross-validation using accuracy as the classification metric. Finally, we used the best estimators of each to determine their accuracies on unseen test data.

# EDA
Plot key distributions for age, education, workclass, race, marital status, and native country. Also look at the income class balance to guide feature choices and any class weighting.

**Figure 1.** Income class distribution (Altair bar chart). Counts for `<=50K` vs `>50K` show the class mix before modeling.

### Baseline and model comparison
Start with a dummy classifier, then compare tree, kNN, SVM, GaussianNB, and Logistic Regression using the shared preprocessing to see which models look promising.

The best perfoming models are Logistic Regression and RBF-SVM, so we tune hyperparameters for those next.

### Hyperparameter tuning for top models
Run randomized search for Logistic Regression (C) and RBF-SVM (C, gamma) with the same preprocessing. Use cross-validated accuracy to pick the settings.

### Model comparison and interpretation
RBF-SVM is a bit more accurate but harder to explain. Logistic Regression is close in accuracy and its coefficients and odds ratios make feature effects clear. Use LR when you need explanations; use SVM if you want the tiny accuracy edge.

## Logistic Regression interpretation

Lets use Logistic Regression coefficients to rank features.
### Visual checks for interpretability
Altair bars show the strongest positive and negative LR coefficients (Figure 2). Binned probability curves show average P(`>50K`) across deciles for `hours-per-week`, `capital-gain`, and `age` with 10th/90th percentile bands (Figure 3).

### Results and Discussion
From the analyses above, we discovered that logistic regression and SVM RBF are the best classification models for predicting income, among all the other classification algorithms. The Logistic Regression test achieves test accuracy of $0.848$ and the RBF-SVM achieves test accuracy of $0.855$. This is somewhat surprising because logistic regression is a baseline model for classifications.

From the logistic regression coefficients, we find that key features include capital gains, occupations, native countries, and relationships, are especially related to determining income levels.

For our next step, we should potentially prune some other irrelevant features to reduce noise in the models and achieve a better result. We can achieve this analyzing Pearson and Kendall correlation coefficients, PCA, or factor analysis (@adhithya2025income). Other ideas include grouping rare country categories to make the model steadier and easier to explain.

## References


